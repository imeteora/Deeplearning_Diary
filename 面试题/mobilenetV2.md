# MobileNetV2

paper https://arxiv.org/abs/1801.04381

MobileNetV2 是由 谷歌 在 2018 年提出，相比 V1，准确率更好，模型更小

模型亮点：

* 1. Inverted Residuals（倒残差结构）

* 2. Linear bottlenecks

###  倒残差结构

一看到 残差，就想到 resnet 了，先来看看 resnet 的 residual block





![img](https://img2020.cnblogs.com/blog/1603920/202003/1603920-20200328105018643-407641202.png)





我们看右边的 block，在 resnet 中 这个 block 专为 深层网络设计，可大大减少 参数量；它先通过 1x1 把 256 channel 变成 64 做降维，然后是 3x3 conv，再接着 1x1 还原为 256 channel 做升维；

总结一句就是先降维再升维，两头胖，中间瘦；

而 倒残差结构就是 两头瘦，中间胖；

如下图





![img](https://img2020.cnblogs.com/blog/1603920/202003/1603920-20200328104150796-1753778775.png)

在具体网络设计时，经过试验，作者并不是把 所有 block 都加上了 残差结构，所以 V2 中有两种 block；



![img](https://img2020.cnblogs.com/blog/1603920/202003/1603920-20200328133236114-853957794.png)





只有 Stride=1 时，才有 残差结构； 

### Linear bottlenecks





我们看到上面的 残差结构 末尾 是 Linear 而不是 Relu，为什么呢？

作者经过研究，发现在 V1 中 depthwise 中有 0 卷积的原因就是 Relu 造成的，换成 Linear 解决了这个问题；

#### Relu 为什么被换掉

作者做了如下实验





![img](https://img2020.cnblogs.com/blog/1603920/202003/1603920-20200328134412385-936349062.png)





作者把一个 二维 流形 数据【图1】 作为 输入， 然后 用不同维度的矩阵 T 把它映射到 高维，然后经过 Relu 输出，再把 输出 经过 T-1 还原成 二维，发现，在 低维 【图2 3】 时，Relu 对信号的损失非常大，随着维度增加，损失越来越小；

结论就是 Relu 对低维信号 损失很大；

回想一下 深度卷积，深度卷积是单通道卷积，只有 1 维，经过 relu 后，即使是 融合后再 Relu 也只有 3 维，信号损失很大，如果学到的信号完全没有用，那就不用学了， w 自然是 0 ；

***\**\*问题又来了，既然 depthwise 时 relue 造成了信号损失，为什么不换 DW 的 relu，而是把 逐点卷积 的 激活函数 换成 Linear 了？\*\*****

* 首先 dw 的 relu 造成 大量损失是因为 Input 的 channel 太少了，如果 Input 的 channel 不少呢，就不损失了，那就没问题了，所以重点不是换不换 relue，是解决 低 channel 的问题；

*  既然 channel 少，我给你增加 channel 不就好了，就是 1x1 升维咯；

* 升完了，我再降回来，将回来之后， channel 就少了，那就换成 linear 咯；

* 于是 倒残差结构 形成咯，再就是画道线的事，参考 resnet 而已；

###  V2 网络结构



![img](https://img2020.cnblogs.com/blog/1603920/202003/1603920-20200328141938502-2069448710.png)

### V2 模型效果



![img](https://img2020.cnblogs.com/blog/1603920/202003/1603920-20200328143535644-1749111925.png) 



一句话，又好又快 

### 主要改进点

\* 引入残差结构，先升维再降维，增强梯度的传播，显著减少推理期间所需的内存占用。

\* 去掉Narrow layer后的RELU，保持特征多样性，增强网络的表达能力。

\* 网络为全卷积的，使得模型可以适应不同尺寸的图像，使用RELU6，最高输出为6激活函数，是的模型在低精度计算下具有更强的鲁棒性



 ### 2、和MobileNetV1 的区别

![在这里插入图片描述](https://img-blog.csdn.net/20181011145544730)



### 3、和 ResNet 的区别



  ![在这里插入图片描述](https://img-blog.csdn.net/2018101114564733)