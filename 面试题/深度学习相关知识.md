###  1X1卷积和作用

* 当1*1卷积核的出现时，在大多数情况下他的作用是升/降特征的维度，这里的维度指的是通道数(厚度)，而不改变图片的宽和高。

举个例子，比如某次卷积之后的结果是WxHx6的特征，现在需要用1x1的卷积核将其降维成Wxhx5，即6通道变成了5通道。

* 减少卷积核参数（简化模型），对于单通道feature map用单核卷积即为乘以一个参数，而一般情况都是多核卷积多通道，实现多个feature map的线性组合。

* 可以实现和全连接层等价的效果。如在faster-rcnn中用1x1xm的卷积核卷积n个特征图的每个位置（像素点），其实对于每个位置的1*1卷积本质上都是对该位置上n个通道组成的n维vector的全连接操作。

对于单通道的feature map和单个卷积核之间的卷积来说，1x1卷积核就是对输入的一个比例缩放，因为1x1卷积核只有一个参数，这个核在输入上滑动，就相当于给输入乘以一个系数。对于多通道而言又一个重要的功能，就是可以保持feature map尺寸不变的前提下大幅度增加非线性特性，把网络做的很deep，CNN里面的卷积大都是多通道的feature map和多通道的卷积核之间的操作（输入的多通道的feature map和一组卷积核做卷积求和得到一个输出的feature map），如果使用1x1的卷积核，这个操作实现的就是多个feature map的线性组合，可以实现feature map在通道个数上的变化。接在普通的卷积层的后面，配合激活函数，就可以实现network in network的结构了。



> # 深度学习面试题——感受野
>
> https://zhuanlan.zhihu.com/p/54011524
>
> https://blog.csdn.net/qq_36653505/article/details/83473943

感受野的定义如下：

卷积神经网络每一层输出的特征图（feature map）上的像素点在原始图像上映射的区域大小；通俗点说，就是图像最终输出的每一个特征到受到原始图像哪一部分的影响

### 感受野的计算

感受野计算时有下面几个知识点需要知道：

* 最后一层（卷积层或池化层）输出特征感受野的大小等于卷积核的大小

* 第i层卷积层的感受野大小和第i层的卷积核大小和步长有关系，同时也与第（i+1）层感受野大小有关。

* 计算感受野的大小时忽略了图像边缘的影响，即不考虑padding的大小。

  关于感受野大小的计算方式是采用从最后一层往下计算的方法，即先计算最深层在前一层上的感受野，然后逐层传递到第一层，使用的公式可以表示如下：

  ![在这里插入图片描述](https://img-blog.csdnimg.cn/20181029112816262.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NjUzNTA1,size_27,color_FFFFFF,t_70)

  其中，是第i层卷积层的感受野，是（i+1）层上的感受野，stride是卷积的步长，Ksize是本层卷积核的大小。

  注意： 此公式与上边的递归公式在原理上是一致的，一个向前计算一个向后计算。
  



> # CNN卷积层、全连接层的参数量、计算量
>
> https://zhuanlan.zhihu.com/p/77471991





# CNN相关介绍

## 内容主要包含：

1：卷积神经网络的结构

2：卷积神经网络的计算

3：卷积神经网络的参数量和计算量计算

### 卷积神经网络的结构

CNN由输入层、卷积层、激活函数、池化层、全连接层组成，即INPUT（输入层）-CONV（卷积层）-BN（归一化）-RELU（激活函数）—POOL（池化层）-FC（全连接层）



![在这里插入图片描述](https://img-blog.csdnimg.cn/20190528164050923.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzQyMjM5Nzk3,size_16,color_FFFFFF,t_70)

### 卷积神经网络的计算

卷积神经网络的计算公式为：

W为输入大小，F为卷积核大小，P为填充大小（padding），S为步长（stride），N为输出大小。有如下计算公式：

​														N = (W-F+2P)/S + 1

```
   nn.Conv2d(in_channels=3,out_channels=96,kernel_size=11,stride=4,padding=2)
```



### 卷积层、全连接层的参数量、计算量

以VGG-16为例，来探讨一下如何计算卷积层、全连接层的参数量、计算量。这里为了简单方便理解，都省略了偏置项。



![img](https://pic3.zhimg.com/80/v2-05cfd1aad3d7db09a5978c375c11eeaa_720w.jpg)

#### 卷积层的参数量

什么是卷积层的参数？

卷积层的基本原理就是图像的二维卷积，即将一个二维卷积模板先翻转（旋转180），再以步长stride进行滑动，滑动一次则进行一次模板内的对应相乘求和作为卷积后的值。

那谁是参数呢？图像本身有参数吗？不可能。

其实只有filter才有参数一说：

![img](https://pic2.zhimg.com/80/v2-afb11fdb9a8bfd48e7cabe79458ccfa5_720w.png)

如上图所示，就是一个 ![[公式]](https://www.zhihu.com/equation?tex=3%5Ctimes3) 的卷积核，它的参数就是9个。

如果有多个通道呢？这简单啊，就用单通道卷积核的参数量乘以通道数呗。

这仅仅是一个filter的参数量，卷积层有若干个filter啊？

（重点，重点，重点）

**计算公式：参数量=（filter size*前一层特征图的通道数）x 当前层filter数量**

VGG-16为例，Conv1-1，输入 ![[公式]](https://www.zhihu.com/equation?tex=224%5Ctimes224%5Ctimes3) ，64个 ![[公式]](https://www.zhihu.com/equation?tex=3%5Ctimes3) filter，输出feature map ![[公式]](https://www.zhihu.com/equation?tex=224%5Ctimes224%5Ctimes64) 

Conv1-1的参数量为 ![[公式]](https://www.zhihu.com/equation?tex=3%5Ctimes3%5Ctimes3%5Ctimes64%3D1728) 。

Conv2-1，输入 ![[公式]](https://www.zhihu.com/equation?tex=112%5Ctimes112%5Ctimes64) ，128个 ![[公式]](https://www.zhihu.com/equation?tex=3%5Ctimes3) filter，输出feature map ![[公式]](https://www.zhihu.com/equation?tex=112%5Ctimes112%5Ctimes128) 。

Conv2-1的参数量 ![[公式]](https://www.zhihu.com/equation?tex=3%5Ctimes3%5Ctimes64%5Ctimes128%3D73728) 。

同理，可以计算其它卷积层的参数量。感兴趣的同学可以自己动手算一下。

#### 全连接层的参数量

上面已经说过卷积层的参数量计算方法了，那**如何计算全连接层的参数量**？其实**和卷积层参数量的计算方法是一样的**。

VGG-16最后一次卷积得到的feature map为 ![[公式]](https://www.zhihu.com/equation?tex=7%5Ctimes7%5Ctimes512) ，全连接层是将feature map展开成一维向量 ![[公式]](https://www.zhihu.com/equation?tex=1%5Ctimes4096) 。**实际上，我们就是用4096个![[公式]](https://www.zhihu.com/equation?tex=7%5Ctimes7%5Ctimes512) 的filter去做卷积**（可以理解为是一个卷积层）。

我们就可以计算第一个FC的参数量 ![[公式]](https://www.zhihu.com/equation?tex=7%5Ctimes7%5Ctimes512%5Ctimes4096%3D102760448) 。

1亿啊，这个数字好大的。这也是为什么说：**全连接层参数冗余**。全连接层参数就可占整个网络参数80%左右，好吓人的。

#### 卷积层的计算量

一次卷积的计算量，如何计算呢？

以VGG-16为例，Conv1-1，输入 ![[公式]](https://www.zhihu.com/equation?tex=224%5Ctimes224%5Ctimes3) ，64个 ![[公式]](https://www.zhihu.com/equation?tex=3%5Ctimes3) filter，输出feature map ![[公式]](https://www.zhihu.com/equation?tex=224%5Ctimes224%5Ctimes64) 

feature map中的每一个像素点，都是64个 ![[公式]](https://www.zhihu.com/equation?tex=3%5Ctimes3) filter 共同作用于原图计算一次得到的，所以它的计算量为 ![[公式]](https://www.zhihu.com/equation?tex=3%5Ctimes3%5Ctimes64%3D576) 。

已经知道单个像素的计算量，那乘以feature map所有像素，就是一次卷积的计算量： ![[公式]](https://www.zhihu.com/equation?tex=224%5Ctimes224%5Ctimes64%5Ctimes3%5Ctimes3%5Ctimes64) 

（重点，重点，重点）

**计算公式：计算量 = 输出的feature map \* 当前层filter**

这仅仅是单个样本前向传播计算量，**实际计算量还应乘以batch size**。

#### 全连接层的计算量

全连接层的计算量，如何计算呢？其实**和卷积层计算量的计算方法是一样的**。

VGG-16最后一次卷积得到的feature map为 ![[公式]](https://www.zhihu.com/equation?tex=7%5Ctimes7%5Ctimes512) ，全连接层是将feature map展开成一维向量 ![[公式]](https://www.zhihu.com/equation?tex=1%5Ctimes4096) 。则FC层的计算量为![[公式]](https://www.zhihu.com/equation?tex=7%5Ctimes7%5Ctimes512%5Ctimes1%5Ctimes4096%3D102760448) 。

#### 总结

通过以上讨论可以发现：我们需要减少网络参数时主要针对全连接层；进行计算优化时，重点放在卷积层。

