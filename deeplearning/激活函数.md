# 几种激活函数的简介



## sigmoid函数



函数公式和图标如下图



![sigmod函数公式](https://img-blog.csdn.net/20180104112208199?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2FuZ3lpNDEx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)





![sigmod函数图](https://img-blog.csdn.net/20180104111804326?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2FuZ3lpNDEx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)



在sigmoid函数中我们可以看到，其输出是在(0,1)这个开区间内。



缺点：



1）当输入稍微远离了坐标原点，函数的梯度就变得很小了，几乎为0。在神经网络反向传播的过程中，我们都是通过微分的链式放着来计算各个权重w的微分的。当反向传播经过了sigmoid函数，这个链条上的微分就很小很小了，况且还可能经过多个sigmoid函数，最后会导致权重w对损失函数几乎没有影响，这样不利于权重的优化，这个问题叫做梯度饱和，也可以叫梯度弥散



2）函数输出不是以0位中心的，这样会使权重更新效率降低



3）sigmoid函数要进行指数运算，这个对于计算机来说是比较慢的



##  tanh函数



tanh函数公式和曲线如下



![tanh函数公式](https://img-blog.csdn.net/20180104112848849?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2FuZ3lpNDEx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)



![tanh函数图](https://img-blog.csdn.net/20180104113045182?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2FuZ3lpNDEx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)





tanh是双曲线正切函数，tanh函数和sigmoid函数的曲线是比较相近的，咱们来比较一下看看。首先相同的是，这两个函数在输入很大或者很小的时候，输出都几乎平滑，梯度很小，不利于权重更新；不同的是输出区间，tanh的输出区间在（-1,1）之间，而且整个函数以0位中心的，这个特点比sigmoid好



一般分类问题中，隐藏层用tanh函数，输出层用simoid函数。不过这些也都不是一成不变的。具体使用什么函数，还是要根据具体问题来具体分析。



##  relu函数



relu函数公式和曲线如下



![relu函数公式](https://img-blog.csdn.net/20180104113836278?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2FuZ3lpNDEx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)



![relu函数图](https://img-blog.csdn.net/20180104114009780?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva2FuZ3lpNDEx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)







relu函数是目前比较火的一个激活函数，相当于sigmoid函数和tanh函数，他有以下几个优点：



1）在输入为正数的时候，不存在梯度饱和问题



2）计算速度要快很多。relu函数只有线性关系，不管是前向传播还是后向传播，都比simoid和tanh函数要快很多。



缺点是：



1）当输入是负数的时候，relu是完全不被激活的，这就表明一单输入到了负数，relu就会死掉，这样在前向传播过程中，还不算什么问题，有的区域是敏感的，有的是不敏感的。但是到了反向传播过程中，输入负数，梯度就会完成到0，这个和simoid函数，tanh函数有一样的问题



2）我们发现relu函数的输出要么是0，要么是正数，这也就是说，relu函数也不是以0位中心的函数



## LeaklyRelu



![这里写图片描述](https://img-blog.csdn.net/20180422215128864?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2Vkb2dhd2FjaGlh/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)



为了解决上述的dead ReLU现象。这里选择一个数，让负数区域不在饱和死掉。这里的斜率都是确定的。



## 应用中如何选择合适的激活函数



1）深度学习旺旺需要大量时间来处理大量数据，模型的收敛速度是尤为重要的，所以，总体来将，训练深度学习网络精良使用zero-centered数据和zero-centered输出，所以尽量选择输出具有zero-centered特点的函数以加快模型的收敛速度



2）如果使用relu，那么一定要小心使用learning rate，而且要注意不要让网络出现很多dead神经元，这个问题不好解决，可以试试leakyrelu



3）最好不要用simoid





# sigmoid和softmax 区别是什么？



**应用Sigmoid函数或Softmax函数**

神经网络分类器最终结果为某一向量，即“原始输出值”，如[-0.5, 1.2, -0.1, 2.4]，这四个输出值分别对应胸部X光检查后发现的肺炎、心脏肥大、瘤和脓肿。但这些原始输出值是什么意思？



将输出值转换为概率可能更容易理解。比起看似随意的“2.4”，患有糖尿病的可能性为91％，这种说法更便于患者理解。



Sigmoid函数或Softmax函数可以将分类器的原始输出值映射为概率。



下图显示了将前馈神经网络的原始输出值（蓝色）通过Sigmoid函数映射为概率（红色）的过程：



![img](https://pics4.baidu.com/feed/730e0cf3d7ca7bcb7046b6d93a686b66f724a8e9.jpeg?token=918acd0d4d9baaf87823957e1009d686&s=5212A02B1340514F94F40CDA0000C0B0)



然后采用Softmax函数重复上述过程：



![img](https://pics7.baidu.com/feed/b21c8701a18b87d6da6b0d3b8269283d1f30fd54.jpeg?token=800a5c72a627ed310a67537b1b71c024&s=5612A62B1B40514F9C7C1CDA0000C0B1)



如图所示，Sigmoid函数和Softmax函数得出不同结果。



原因在于，Sigmoid函数会分别处理各个原始输出值，因此其结果相互独立，概率总和不一定为1，如图0.37 + 0.77 + 0.48 + 0.91 = 2.53。



相反，Softmax函数的输出值相互关联，其概率的总和始终为1，如图0.04 + 0.21 + 0.05 + 0.70 = 1.00。因此，在Softmax函数中，为增大某一类别的概率，其他类别的概率必须相应减少。





## 总结



如果模型输出为非互斥类别，且可以同时选择多个类别，则采用Sigmoid函数计算该网络的原始输出值。



如果模型输出为互斥类别，且只能选择一个类别，则采用Softmax函数计算该网络的原始输出值。