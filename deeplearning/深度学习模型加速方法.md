

## 深度学习模型加速方法

### 参考链接

* [如何让你的yoloV3模型更小更快？](https://mp.weixin.qq.com/s/nqEXUBXHedNImabZboSk3Q)
* 

### 深度学习模型加速方法

#### 简介

* 模型压缩算法，能够有效降低参数冗余，从而减少存储占用，通信带宽和计算复杂度，有助于深度学习的应用部署，具体可以划分为剪枝和量化

  1：线性或非线性量化：int8和fp16等

  2：结构或非结构剪枝：deep compression, channel pruning 和 network slimming等；

  3：其他，权重矩阵的低秩分解，知识蒸馏与网络结构简化（squeeze-net，mobilenet，shufflenet）

* 模型优化加速能够提升网络的计算效率，具体包括

  1：优化工具与库，TensorRT openvino等



#### 网络剪枝

深度学习模型因其稀疏性或过拟合倾向，可以被剪枝为结构精简的网络模型，具体包括结构剪枝和非结构剪枝：

* 非结构剪枝：通常是链接级，细粒度的剪枝方法，精度相对较高，但依赖于特征算法库或硬件平台的支持，如Deep Compression [5], Sparse-Winograd [6] 算法等；

* 结构剪枝：是filter或layer级，粗粒度的剪枝方法，精度相对较低，但剪枝策略更为有效，不需要特定算法库或硬件平台的支持，能够直接在成熟深度学习框架上运行。

  模型剪枝的难点在于对“不重要”的参数的定义和最优剪枝结构的搜索方法。目前主流的做法是训练一个大模型，然后根据参数权重的大小对大模型进行剪枝，去除不重要的参数，最后再对剪之后的模型finetune一下，但是这种方法收敛比较慢，而且最终得到的模型不一定是最优的。

  #### 模型量化

  模型量化是通过减少表示每个权重参数所需的比特数来压缩原始网络，从而实现计算加速。

  半浮点精度（fP16）和混合精度是一种常见的做法，不过需要底层计算框架支持，否则无法实现计算加速。另一种是int8量化，即将模型的权重参数从FP32转换为int8，以及使用int8进行推理，量化的加速主要得益于顶点运算比浮点运算快，但从FP32量化为int8会损失模型精度。

  #### 知识蒸馏

  模型蒸馏本质上和迁移学习类似，知识它还多了一个模型压缩的目的，即通过模型蒸馏，将大模型压缩为小模型，使小模型可以跑得又快又好，所以，最基本的想法就是大模型学习得到的知识作为先验知识，将先验知识传递给下模型的神经网络中，并在实际应用部署小规模的神经网络。