*  [深度学习调参有哪些技巧？](https://www.zhihu.com/question/25097993)

## 做工程

\* 3x3卷积CNN的主流组件。平时有设计一些解决分类，回归任务的网络，里面的卷积核基本都设置为3x3，要说原因的话应该去问问VGG16吧。两个3x3的卷积核堆叠能获得5x5卷积核的感受野并且参数比5x5卷积核少，所以是大量推荐使用的。

\* 可以适当使用1xN卷积，为什么要提升这一点呢？这是因为1xN卷积可以减少计算量，并且1xN卷机可以在某个方向强调感受野，也就是说假如如果你要对一个长方形的目标进行分类，你可以使用1xN的卷积核搭配3x3的卷积核对长边方向设定更大的感受野，或许可以获得泛化性能的提升。

\* 卷积核权重初始化方式。对weight的初始化我一般都是使用xavier初始化。

\* Batch Normalization. 这是我一直使用的技巧，可以很大程度的加快收敛速度，建议搭建自己网络的时候尽量加上BN，如果有BN了全连接层没必要加Dropout了。

\* 目标检测不能盲目去掉fpn结构。在针对自己的数据调检测任务如yolov3的时候不能盲目砍掉fpn结构，尽管你分析出某个分支的Anchor基本不可能会对你预测的目标起作用，但如果你直接去掉分支很可能会带来漏检。

\* 优化器的选择，我基本上都是带动量的SGD，如果优化不动可以试试Adam。

\* 激活函数。可以先用RELU，如果想再提升精度可以将relu改成prelu试试。

\* batch size：在不同类型的任务中，batch_size的影响也不同。

\* 初始化学习率，一般是从0.01开始设置，我个人认为这个学习率和学习率衰减策略是相关的，但不宜设置的过大过小，0.01和0.1应该是比较常用的。学习率衰减策略我一般使用multistep方式，step_size的设置要看视你的的max_iter而定。

\* 数据与处理之zero-center

\* 残差结构和密集链接。

\* 关于loss，优秀loss一般是对模型的泛化性能有所改善的。但是用loss的时候往往并不是直接替换loss那么简单，需要仔细思考loss背后的数学原理，要用对地方才可有提升，例如，如果将focal loss用到yolov3中提升map.

\* 找到模型调参时的可靠评价指标，在调参训练模型时一定要找到正确的评价指标，每调整一个参数就要记录一下模型的评价指标如准确率，map值，miou值等。并且在调参时建议将调整的参数和在测试集上的精度组合成一个字符串给模型重命令，方便之后快速review

\* 使用带backone的网络，如训练VGG16-ssd建议选择finetune的方式，从头训练不仅耗时耗力，甚至难以收敛。

\*在做分割实验的时候我发现用unsampleing加1x1卷积代替反卷积坐上采样得到的结果更平滑，并且miou差距不大，所以我认为这两者都是可以使用的。

\## 简短的注意事项

\* 预处理，-mean/std zero-center就够了，pca，白化什么的都用不上

\* shuffle，shuffle，shuffle

\* 网络原理的理解最重要，CNN的conv这块，你的明白sobel算法的边界预测

\* Dropout，Dropout，Dropout（不仅仅可以防止过拟合，其实这相当于成本最低的ensemble，当然训练起来会比没有Dropout要慢一点，同事网络参数你最好响应加一点）

\* CNN更加适合训练回答是否的问题，如果任务比较复杂，考虑先用分类任务训练一个模型再finetune

\* 无脑用relu

\* 无脑用3*3

\* 无脑用xavier

\* LRN一类的，其实可以不用，不行可以在拿来试试看

\* filter数量2^n 

\* 多尺度的图片输入（或者网络内部利用多尺度下的结果）又很好的提升效果

\* 第一层的filter，数量不要太少，否则根本学不出来，底层特征很重要

\* sgd adam，优化器上，一般采用sgd+momentum

\* BN 



# 方式2

作者：时间旅客

链接：https://www.zhihu.com/question/41631631/answer/1129785528

不管什么模型，先在一个较小的训练集上train和test，看看它能不能过拟合。如果不能过拟合，可能是学习率太大，或者代码写错了。先调小学习率试一下，如果还不行就去检查代码，先看dataloader输出的数据对不对，再看模型每一步的size是否符合自己期待。

看train/eval的loss曲线，正常的情况应该是train loss呈log状一直下降最后趋于稳定，eval loss开始时一直下降到某一个epoch之后开始趋于稳定或开始上升，这时候可以用early stopping保存eval loss最低的那个模型。如果loss曲线非常不正常，很有可能是数据处理出了问题，比如label对应错了，回去检查代码。

不要一开始就用大数据集，先在一个大概2w训练集，2k测试集的小数据集上调参。

尽量不要自己从头搭架子（新手和半新手）。找一个已经明确没有bug能跑通的其它任务的架子，在它的基础上修改。否则debug过程非常艰难，因为有时候是版本迭代产生的问题，修改起来很麻烦。

优化器优先用adam，学习率设1e-3或1e-4，再试Radam（[LiyuanLucasLiu/RAdam](http://link.zhihu.com/?target=https%3A//github.com/LiyuanLucasLiu/RAdam)）。不推荐sgdm，因为很慢。

lrscheduler用torch.optim.lr_scheduler.CosineAnnealingLR，T_max设32或64，几个任务上试效果都不错。（用这个lr_scheduler加上adam系的optimizer基本就不用怎么调学习率了）

有一些任务（尤其是有RNN的）要做梯度裁剪，torch.nn.utils.clip_grad_norm。

参数初始化，lstm的h用orthogonal，其它用he或xavier。

激活函数用relu一般就够了，也可以试试leaky relu。

batchnorm和dropout可以试，放的位置很重要。优先尝试放在最后输出层之前，以及embedding层之后。RNN可以试layer_norm。有些任务上加了这些层可能会有负作用。

metric learning中先试标label的分类方法。然后可以用triplet loss，margin这个参数的设置很重要。

batchsize设置小一点通常会有一些提升，某些任务batchsize设成1有奇效。

embedding层的embedsize可以小一些（64 or 128），之后LSTM或CNN的hiddensize要稍微大一些（256 or 512）。（ALBERT论文里面大概也是这个意思）

模型方面，可以先用2或3层LSTM试一下，通常效果都不错。

weight decay可以试一下，我一般用1e-4。

有CNN的地方就用shortcut。CNN层数加到某一个值之后对结果影响就不大了，这个值作为参数可以调一下。

GRU和LSTM在大部分任务上效果差不多。

看论文时候不要全信，能复现的尽量复现一下，许多论文都会做低baseline，但实际使用时很多baseline效果很不错。

对于大多数任务，数据比模型重要。面对新任务时先分析数据，再根据数据设计模型，并决定各个参数。例如nlp有些任务中的padding长度，通常需要达到数据集的90%以上，可用pandas的describe函数进行分析。